# -*- coding: utf-8 -*-
"""DS_RF.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/10Bl_GtitAczLvf-Y8m8rC2BSvebge5Op
"""

!pip install pandas scikit-learn matplotlib seaborn --quiet

import pandas as pd
import numpy as np
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import train_test_split
from sklearn.model_selection import GridSearchCV, train_test_split
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score

"""ACES DATASET

"""

from scipy.io import loadmat
import pandas as pd
import numpy as np

base_directory = '.'
X = pd.DataFrame(loadmat(f'/content/DATASET/ACESExpr.mat')['data'])
y = loadmat(f'/content/DATASET/ACESLabel.mat')['label'].reshape(-1)
subtypes = np.loadtxt(f'/content/DATASET/ACES_Subtype.txt').astype(int)
entrez_gene_id = loadmat(f'/content/DATASET/ACES_EntrezIds.mat')['entrez_ids']
X.columns = entrez_gene_id.reshape(-1)

"""**RF Simple**"""

# Convert X (DataFrame) to 2D NumPy array
X_np = X.values.astype(float)  # Shape: (1616, 12750)

# Convert y (already 1D from reshape) to int type
y_np = y.astype(int)           # Shape: (1616,)

# Print shapes to verify
print("X shape:", X_np.shape)
print("y shape:", y_np.shape)

# Preview first 2 samples (rows) and first 5 genes (columns)
print("\nX sample (first 2 rows, first 5 genes):")
print(X_np[:2, :10])

# Preview first 10 labels
print("\ny sample (first 10 labels):")
print(y_np[:10])

# Step 2: Split the data
X_train, X_test, y_train, y_test = train_test_split(
    X_np, y_np, test_size=0.2, random_state=42, stratify=y_np
)

# Step 3: Define the Random Forest and hyperparameter grid
rf = RandomForestClassifier(random_state=42, n_jobs=-1)

param_grid = {
    'n_estimators': [50, 100, 200],
    'max_depth': [10, 20, None],
    'min_samples_split': [2, 5, 7],
    'min_samples_leaf': [1, 2, 3],
    'max_features': ['sqrt', 'log2'],
    'class_weight': ['balanced']
}

# Step 4: Set up GridSearchCV
grid_search = GridSearchCV(
    estimator=rf,
    param_grid=param_grid,
    cv=5,
    n_jobs=-1,
    scoring='accuracy',
    verbose=1
)

# Step 5: Fit the model
grid_search.fit(X_train, y_train)

# Step 6: Get the best model
best_rf = grid_search.best_estimator_
print("Best Hyperparameters:", grid_search.best_params_)

# Step 7: Evaluate on test set
y_pred = best_rf.predict(X_test)
print("\nClassification Report:\n", classification_report(y_test, y_pred))
print("Accuracy:", accuracy_score(y_test, y_pred))

# Step 8: Confusion Matrix
conf_matrix = confusion_matrix(y_test, y_pred)
plt.figure(figsize=(6, 4))
sns.heatmap(conf_matrix, annot=True, fmt="d", cmap="Blues", xticklabels=["No Metastasis", "Metastasis"], yticklabels=["No Metastasis", "Metastasis"])
plt.xlabel("Predicted")
plt.ylabel("True")
plt.title("Confusion Matrix")
plt.show()

"""**RF with SMOTE**"""

from sklearn.model_selection import train_test_split
from imblearn.over_sampling import SMOTE
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import classification_report

# Step 1: Stratified split
X_train, X_test, y_train, y_test = train_test_split(
    X_np, y_np,
    test_size=0.2,
    stratify=y_np,
    random_state=42
)

# Step 2: Oversample minority class
sm = SMOTE(random_state=42)
X_train_res, y_train_res = sm.fit_resample(X_train, y_train)

# Step 3: Train with class weighting (optional, you can skip it since SMOTE balances)
model = RandomForestClassifier(
    n_estimators=100,
    class_weight='balanced',
    random_state=42,
    n_jobs=-1
)
model.fit(X_train_res, y_train_res)

y_pred = best_rf.predict(X_test)
print("\nClassification Report:\n", classification_report(y_test, y_pred))
print("Accuracy:", accuracy_score(y_test, y_pred))
# Step 4: Evaluate
y_pred = model.predict(X_test)
print(classification_report(y_test, y_pred))

"""**RF+Tomek Link**"""

!pip install imbalanced-learn --quiet

from imblearn.combine import SMOTETomek
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import classification_report

# Step 1: Stratified train/test split
X_train, X_test, y_train, y_test = train_test_split(
    X_np, y_np, test_size=0.2, stratify=y_np, random_state=42
)

# Step 2: Apply SMOTE + Tomek Links
smote_tomek = SMOTETomek(random_state=42)
X_resampled, y_resampled = smote_tomek.fit_resample(X_train, y_train)

# Step 3: Train model
model = RandomForestClassifier(class_weight='balanced', random_state=42)
model.fit(X_resampled, y_resampled)

y_pred = best_rf.predict(X_test)
print("\nClassification Report:\n", classification_report(y_test, y_pred))
print("Accuracy:", accuracy_score(y_test, y_pred))
# Step 4: Evaluate
y_pred = model.predict(X_test)
print("SMOTE + Tomek Links")
print(classification_report(y_test, y_pred))

"""Borderline SMOTE + RF"""

from imblearn.over_sampling import BorderlineSMOTE

# Step 1: Stratified split (if not done already)
X_train, X_test, y_train, y_test = train_test_split(
    X_np, y_np, test_size=0.2, stratify=y_np, random_state=42
)

# Step 2: Apply Borderline-SMOTE
border_smote = BorderlineSMOTE(kind='borderline-1', random_state=42)
X_resampled, y_resampled = border_smote.fit_resample(X_train, y_train)

# Step 3: Train model
model = RandomForestClassifier(class_weight='balanced', random_state=42)
model.fit(X_resampled, y_resampled)

y_pred = best_rf.predict(X_test)
print("\nClassification Report:\n", classification_report(y_test, y_pred))
print("Accuracy:", accuracy_score(y_test, y_pred))

# Step 4: Evaluate
y_pred = model.predict(X_test)
print("Borderline-SMOTE")
print(classification_report(y_test, y_pred))